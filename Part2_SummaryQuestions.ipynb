{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 2: Summary Questions\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The receptive field of a unit (neuron) in a CNN is the spacial region of the input that has an affect on the unit. The receptive field of a unit that is not in the first hidden layer is the union of the recptive field of it's input, because it is affected by it. The higher the convolution kernel size, stride and dialation of a layer, the larger the receptive field is.\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "a. *Kernel Size* - The size of the convolution kernel. At each layer, each unit is affected by an amount of units that depends on how large is the kernel size, thus, the larger values of kernel size we use in the network, the higher the rate in which the receptive field grows. \n",
    "The higher the kernel size is, the more features of the input affect each unit of the output, but the extracted features are less regional.\n",
    "\n",
    "\n",
    "b. *Stirde* - Detemines how many input points are skipped between each step of the convolution. This value, has an affect on how an output unit will be similar to it's neighbours becasue they have common inputs. When this value is higher, neighbor units will depand on different inputs, thus when performing convolution again on the output, the next layer will have a higher receptive field.\n",
    "The stride value, does not affect directly the receptive field of a layer but only the receptive field of the next layer.\n",
    "\n",
    "c. *Dialation* - Determines the gap inconvolution within each step.  The higher the dialation, the larger the area the area of the input the kernel \"looks at\", but more input values are ignored between those who are used. Thats means of course that increasing the dialation also increases the receptive field, but some features from the input are lost. A dialation may be similar to pooling + convolution.\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:16.753322Z",
     "iopub.status.busy": "2021-01-26T09:18:16.752610Z",
     "iopub.status.idle": "2021-01-26T09:18:17.987501Z",
     "shell.execute_reply": "2021-01-26T09:18:17.988034Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Every pixel after the first layer has recptive field of 3x3\n",
    "\n",
    "# TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "    \n",
    "By creating residual connection in the network, we force the layer to learn the identity function plus another\n",
    "function of the input in contrast to a regular layer where the network learns a completly new function of the input. therefore, \n",
    "with resudual connection the network is expected to learn different mapping, which will probably have smaller weights since it's\n",
    "initial pretrained output is the input + random function, there fore we will get completly different filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:17.992615Z",
     "iopub.status.busy": "2021-01-26T09:18:17.991868Z",
     "iopub.status.idle": "2021-01-26T09:18:18.013482Z",
     "shell.execute_reply": "2021-01-26T09:18:18.014164Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "p1, p2 = 0.1, 0.2\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=p1),\n",
    "    nn.Dropout(p=p2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to replace the two consecutive dropout layers with a single one defined as follows:\n",
    "```python\n",
    "nn.Dropout(p=q)\n",
    "```\n",
    "what would the value of `q` need to be? Write an expression for `q` in terms of `p1` and `p2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "A value is droped out if it was droped out in the first dropout layer, the second one, or both. \n",
    "We will calculate the probability a value is not droped out:\n",
    "\n",
    "$ 1 - q = (1-p_1)(1-p_2) $\n",
    "\n",
    "$ q = 1 - (1 - p_2 - p_1 + p_1 p_2) = p_1 + p_2 - p_1 p_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Altough, as a rule of thump a dropout is suggested to be put after the activation layer, \n",
    "it can also be put before it, and in the case of a relu it is even more computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Consider an activation vector $\\mathbf{x}$, and the same activation vector $\\mathbf{\\tilde x}$ with dropout applied, which means each \n",
    "\n",
    "$\\mathbf{\\tilde x_i} = \\begin{cases}\n",
    "    x_i & \\text{w.p } (1-p) \\\\ \n",
    "    0 & \\text{w.p } (p)\n",
    "\\end{cases}$ \n",
    "\n",
    "Therefore \n",
    "\n",
    "$\\mathbb{E}[\\mathbf{\\tilde x}] = 1/n \\sum_{i=1}^{n}{\\tilde x_i} = 1/n \\sum_{i=1}^{n}{(1-p)x_i} = (1-p) \\mathbb{E}[\\mathbf{x}]$ \n",
    "\n",
    "The expectation of the activation is :\n",
    "$\\mathbb{E}[\\sum{w_i \\tilde x_i}] = (1-p) \\mathbb{E}[\\sum{w_i x_i}]$ therefore in order to maintain the expectation, we have to scale it by $1/(1-p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://sparrowism.soc.srcf.net/home/piratesarecool4.gif\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.019108Z",
     "iopub.status.busy": "2021-01-26T09:18:18.018447Z",
     "iopub.status.idle": "2021-01-26T09:18:18.042969Z",
     "shell.execute_reply": "2021-01-26T09:18:18.043669Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H), nn.Sigmoid(),\n",
    "    ]*24,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "  1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "  1. The gradient of ReLU is linear with its input when the input is positive.\n",
    "  1. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "    \n",
    "The difference is in the amount of samples used to calculate the gradient in each step.\n",
    "In regular gradient decent we use all of our training samples in each step, that may cause the model to overfit and also be to expensive to calculate, or even impossible due to memory.\n",
    "In stochastic gradient decent we use one sample in each step, that may prevent the model from converging to a minimum because the updates are chaotic. \n",
    "In batch gradient decent we use a fixed small number (usualy 2-1024) of samples in each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding SGD and GD:\n",
    "  1. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "  2. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Two reasons: \n",
    "\n",
    "    a. Calculating the gradients for all the samples may take long time, so each gradient step would take long time and the model would train slow\n",
    "    b. GD tends to overfit the training set while when using SGD the model may have better generalization.\n",
    "\n",
    "GD can't be used at all when there are too mach data samples so the values of the gradients can't be all together in the memory at the same time.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We would expect less iterations to converge. Since we are using more samples in each update steps, the update dircetion would be more accurate and less stochastic, therfore we need less steps to converge since the optimizer moves to a better direction. Altough we would need less steps to converge, it will not necessarily take less time, because at each iteration we have to use more samples to calculate the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "  1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "  1. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "  1. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "  1. Training  with SGD requires more memory than with GD.\n",
    "  1. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "  1. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "5. False, in SGD we sample randomly from the training set at each step, some samples may be sampled twice or more in that proccess while other are not sampled at all\n",
    "\n",
    "6. False, gradients in SGD are only affected by one sample. samples may be different therefore the variance will be higher and convergence will be slower\n",
    "\n",
    "7. True, since in SGD different samples are used in each step, new samples may get the optimizer out of a local minima that caused by other samples\n",
    "\n",
    "8. False, in SGD we don't have to use all samples simultaneously, so we dont have to keep them all in the memory at the same time\n",
    "\n",
    "9. False, GD is not guaranteed to converge to a global minimum. it depends on the initialization.\n",
    "\n",
    "10. True, in Newton's method we  will have to compute the second order derivitives for the hessian, which will take time in each step, while with momentum we will only have to compute the gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In tutorial 5 we saw an example of bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "  **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "    \n",
    "לא הבנתי כלכך, אני בנתיים מעתיק תשובה מרפרנס שמצאתי ולא הבנתי:\n",
    "\n",
    "    False, The inner optimization problem can be solved as a constraint in the network's loss function. Therefore we can minimize an expression $L = f(x) + \\lambda g(x)$ without using a descent based method just like we find a minimum on a regular function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "  1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "  2. How can each of these problems be caused by increased depth?\n",
    "  3. Provide a numerical example demonstrating each.\n",
    "  4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "7. In vanishing gradients, the gradients goes smaller as they propogate through the network until they get close to zero and act like a numeric zero. In exploding gradients, the gradients glow's as they propogate until they get to big, thus the update step is to large so the optimizer can't find the minimum.\n",
    "\n",
    "8. When back propogating the gradient, we multiply the gradient of the previus layer with the gradient of the current layer (according to the chain rule). Increasing the number of layer, will increasce the number of multipications. For gradients lower then 1, the values exponentialy decay to zero, and for gradients higher then 1 the values would exponentialy grow (to infinity)\n",
    "\n",
    "9. Consider a netwrork with one unit in each layer, so the function it represents is: $\\hat y = f_n(f_{n-1}(...f_1(x;\\Theta_1)...;\\Theta_{n-1});\\Theta_n))$ for functions $f_i$ parametrized by $\\Theta_i$ as the layers.\n",
    "\n",
    "    for vanishing gradients, let's assume that $\\frac{\\partial L}{ \\partial \\Theta_n} = \\frac{\\partial f_i}{\\partial \\Theta_i} = 0.9$ Which is a little bit lower then 1.\n",
    "    Therfore, $\\frac{\\partial L}{ \\partial \\Theta_1} = \\frac{\\partial L}{ \\partial \\Theta_n} \\frac{\\partial f_{n-1} }{ \\partial \\Theta_{n-1}} ... \\frac{\\partial f_{1} }{ \\partial \\Theta_{1}} = 0.9^n$. We can see an exponential decay with $n$ which is the number of layer. With 1,000 layers for example, we would get a gradient as low as $10^{-46}$ which is practicly zero.  \n",
    "    \n",
    "    or exploding gradients, let's assume that $\\frac{\\partial L}{ \\partial \\Theta_n} = \\frac{\\partial f_i}{\\partial \\Theta_i} = 1.1$ Which is a little bit higher then 1.\n",
    "    Therfore, $\\frac{\\partial L}{ \\partial \\Theta_1} = \\frac{\\partial L}{ \\partial \\Theta_n} \\frac{\\partial f_{n-1} }{ \\partial \\Theta_{n-1}} ... \\frac{\\partial f_{1} }{ \\partial \\Theta_{1}} = 1.1^n$. We can see an exponential growth with $n$. With 1,000 layers, we would get a gradient as high as $10^{1}$ which is practicly infinity.  \n",
    "    \n",
    "10. We can look at the graph of the loss function. If the loss fluctuates, or diverges, it is probably becasue the gradients are too high, making the optimizer to take too large steps. If the loss is constant and does not change after a lot of steps, the steps are probably too small becasue the gradient is close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}^{(i)}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The derivative of a function $f(\\vec{x})$ at a point $\\vec{x}_0$ is\n",
    "  $$\n",
    "  f'(\\vec{x}_0)=\\lim_{\\Delta\\vec{x}\\to 0} \\frac{f(\\vec{x}_0+\\Delta\\vec{x})-f(\\vec{x}_0)}{\\Delta\\vec{x}}\n",
    "  $$\n",
    "  \n",
    "  1. Explain how this formula can be used in order to compute gradients of neural network parameters numerically, without automatic differentiation (AD).\n",
    "  \n",
    "  2. What are the drawbacks of this approach? List at least two drawbacks compared to AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Given the following code snippet:\n",
    "  1. Write a short snippet that implements that calculates gradient of `loss` w.r.t. `W` and `b` using the approach of numerical gradients from the previous question.\n",
    "  2. Calculate the same derivatives with autograd.\n",
    "  3. Show, by calling `torch.allclose()` that your numerical gradient is close to autograd's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.049487Z",
     "iopub.status.busy": "2021-01-26T09:18:18.048852Z",
     "iopub.status.idle": "2021-01-26T09:18:18.072993Z",
     "shell.execute_reply": "2021-01-26T09:18:18.073616Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, d = 100, 5\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d, dtype=dtype)\n",
    "W, b = torch.rand(d, d, requires_grad=True, dtype=dtype), torch.rand(d, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def foo(W, b):\n",
    "    return torch.mean(X @ W + b)\n",
    "\n",
    "loss = foo(W, b)\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "# TODO: Calculate gradients numerically for W and b\n",
    "# grad_W =...\n",
    "# grad_b =...\n",
    "\n",
    "# TODO: Compare with autograd using torch.allclose()\n",
    "# autograd_W = ...\n",
    "# autograd_b = ...\n",
    "# assert torch.allclose(grad_W, autograd_W)\n",
    "# assert torch.allclose(grad_b, autograd_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding word embeddings:\n",
    "  1. Explain this term and why it's used in the context of a language model.\n",
    "  1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding (i.e. trained directly on sequences of tokens)? If yes, what would be the consequence for the trained model? if no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain:\n",
    "  1. What does `Y` contain? why this output shape?\n",
    "  2. How you would implement `nn.Embedding` yourself using only torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.077958Z",
     "iopub.status.busy": "2021-01-26T09:18:18.077299Z",
     "iopub.status.idle": "2021-01-26T09:18:18.378980Z",
     "shell.execute_reply": "2021-01-26T09:18:18.379850Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of S: State whether the following sentences are **true or false**, and explain.\n",
    "  1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "  2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length S.\n",
    "  3. TBPTT allows the model to learn relations between input that are at most S timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 7 (part 2) we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "  1. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention?\n",
    "  \n",
    "  2. After learning that self-attention is gaining popularity thanks to the shiny new transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections). What influence do you expect this will have on the learned hidden states?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "  1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "  1. Images generated by the model ($z \\to x'$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  2. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction.\n",
    "  3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "  2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "  3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "  5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have implemented a graph convolutional layer based on the following formula, for a graph with $N$ nodes:\n",
    "$$\n",
    "\\mat{Y}=\\varphi\\left( \\sum_{k=1}^{q} \\mat{\\Delta}^k \\mat{X} \\mat{\\alpha}_k + \\vec{b} \\right).\n",
    "$$\n",
    "  1. Assuming $\\mat{X}$ is the input feature matrix of shape $(N, M)$: what does $\\mat{Y}$ contain in it's rows?\n",
    "  1. Unfortunately, due to a bug in your calculation of the Laplacian matrix, you accidentally zeroed the row and column $i=j=5$ (assume more than 5 nodes in the graph).\n",
    "What would be the effect of this bug on the output of your layer, $\\mat{Y}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We have discussed the notion of a Receptive Field in the context of a CNN. How would you define a similar concept in the context of a GCN (i.e. a model comprised of multiple graph convolutional layers)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
