{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf49d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2dd650",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loading the dataset & Pre trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29478764",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchtext.data\n",
    "import torchtext.datasets\n",
    "import numpy as np \n",
    "import time\n",
    "import inspect\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) \n",
    "\n",
    "from cs236781.train_results import FitResult\n",
    "from cs236781 import plot\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device =\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e0526",
   "metadata": {},
   "source": [
    "load the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.sys.platform == 'linux':\n",
    "    data_dir = os.path.expanduser('~/HW4/project/GloVe')\n",
    "else:\n",
    "    data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "    \n",
    "# torchtext Field objects parse text (e.g. a review) and create a tensor representation\n",
    "\n",
    "# This Field object will be used for tokenizing the movie reviews text\n",
    "review_parser = torchtext.data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    init_token='<sos>', eos_token='<eos>', dtype=torch.long,\n",
    "    tokenize='spacy', tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "# This Field object converts the text labels into numeric values (0,1,2)\n",
    "label_parser = torchtext.data.Field(\n",
    "    is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    ")\n",
    "\n",
    "# Load SST, tokenize the samples and labels\n",
    "# ds_X are Dataset objects which will use the parsers to return tensors\n",
    "ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "    review_parser, label_parser, root=data_dir\n",
    ")\n",
    "\n",
    "n_train = len(ds_train)\n",
    "print(f'Number of training   samples: {n_train}')\n",
    "print(f'Number of validation samples: {len(ds_valid)}')\n",
    "print(f'Number of test       samples: {len(ds_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefd1cb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As required, we'll use the pre-trained word embeddings of glove 6B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648a831",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Vocabulary size is 40k, Embedding chosen size in 50\n",
    "vocab, embeddings = [],[]\n",
    "with open('./GloVe/glove.6B.200d.txt','rt',encoding='utf8') as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee24aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "add padding and unknown tokens to the embeddings array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34160baf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add the padding and the unknown tokens to the vocab and embeddings arrays\n",
    "\n",
    "vocab = np.array(vocab) \n",
    "embeddings = np.array(embeddings)\n",
    "vocab = np.insert(vocab, 0, '<pad>')\n",
    "vocab = np.insert(vocab, 1, '<unk>')\n",
    "\n",
    "unk_emb = np.mean(embeddings, axis=0, keepdims=True)\n",
    "pad_emb = np.zeros_like(embeddings[0]).reshape(1,-1)\n",
    "\n",
    "\n",
    "embeddings = np.vstack((pad_emb, unk_emb, embeddings))\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c38ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5483166",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dl_train, dl_test, max_epochs=100,\n",
    "          num_batches=400, print_every=1, save_path=None):\n",
    "    \n",
    "    best_test_acc = 0\n",
    "    res = FitResult(max_epochs,[],[],[],[] )\n",
    "    \n",
    "    for epoch_idx in range(max_epochs):\n",
    "        total_loss, num_correct = 0, 0\n",
    "        num_samples = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dl_train):\n",
    "            X, y = batch.text, batch.label\n",
    "\n",
    "            # Forward pass\n",
    "            _, y_pred_log_proba = model(X)\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred_log_proba, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_samples += y_pred.shape[0]\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "\n",
    "            if batch_idx == num_batches-1:\n",
    "                break\n",
    "        \n",
    "        curr_test_loss, curr_test_acc = test_epoch(model, loss_fn, dl_test, print_acc=(epoch_idx % print_every == 0))\n",
    "        res.test_loss.append(curr_test_loss)\n",
    "        res.test_acc.append(curr_test_acc)\n",
    "        \n",
    "        curr_train_loss = total_loss /(num_batches)\n",
    "        curr_train_acc = num_correct /(num_samples)\n",
    "        res.train_loss.append(curr_train_loss)\n",
    "        res.train_acc.append(curr_train_acc)\n",
    "        \n",
    "        if epoch_idx % print_every == 0:\n",
    "            print(f\"Epoch #{epoch_idx}, loss={curr_train_loss:.3f}, accuracy={curr_train_acc:.3f}, elapsed={time.time()-start_time:.1f} sec\")\n",
    "        \n",
    "        if save_path and curr_test_acc > best_test_acc:\n",
    "            if epoch_idx % print_every == 0:\n",
    "                print(\"---saving model ---\")\n",
    "            torch.save(model, save_path)\n",
    "            best_test_acc = curr_test_acc\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfd22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, loss_fn, dataloader, print_acc=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_correct = 0 \n",
    "    num_batches = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            num_batches = batch_idx\n",
    "            X, y = batch.text, batch.label\n",
    "\n",
    "            _, y_test = model(X)\n",
    "            loss = loss_fn(y_test, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            y_pred = torch.argmax(y_test, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "            num_samples += y_pred.shape[0]\n",
    "\n",
    "        num_batches += 1   \n",
    "        \n",
    "        test_loss = total_loss /(num_batches)\n",
    "        test_acc = num_correct /(num_samples)\n",
    "        if print_acc:\n",
    "            print(\"Test Accuracy is ----\", test_acc, \"-----\")\n",
    "\n",
    "    model.train()\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c21837",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline Model - Sentiment Analysis using RNN - GRU\n",
    "\n",
    "As for the first part in our experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c71725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "HIDDEN_SIZE = 100\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "FREEZE_EMBEDDINGS = False\n",
    "\n",
    "LOSS_WEIGHTS = [1., 1., 1.]\n",
    "\n",
    "WEIGHT_DECAY = 0  # 1e-5\n",
    "LEARNING_RATE = 2e-4\n",
    "BETAS = (0.95, 0.98)  # this is the best for stable training, default is (0.99, 0.999), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814bda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, device=device)\n",
    "review_parser.build_vocab(ds_train)\n",
    "label_parser.build_vocab(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the baseline model\n",
    "from RNN import SentimentGRU\n",
    "\n",
    "model = SentimentGRU(embeddings,\n",
    "                     hidden_size=HIDDEN_SIZE,\n",
    "                     num_layers=NUM_LAYERS,\n",
    "                     dropout=DROPOUT,\n",
    "                     freeze_embedding=FREEZE_EMBEDDINGS).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=WEIGHT_DECAY,\n",
    "                             betas=BETAS)\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f6c29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fit_res = train(model, optimizer, loss_fn, dl_train, dl_test, max_epochs=100,\n",
    "      num_batches=500, save_path = \"./models/sentimentGRU.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9ebc3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot.plot_fit(fit_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model achieved during training\n",
    "model = torch.load(\"./models/sentimentGRU.pt\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6180f41",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Self Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attenttion Hyper parameters:\n",
    "\n",
    "BATCH_SIZE = 128 # best so far\n",
    "NUM_HEADS = 1 # does not matter alot\n",
    "D_MODEL = 50 \n",
    "TWO_LAYERS = False\n",
    "FREEZE_EMB = False # weather to fine tune the embedding\n",
    "DROPOUT = 0.8 # best with the combination of weight decay\n",
    "KQV_DROPOUT = 0 # no dropout there in the paper\n",
    "DENSE_DROPOUT = 0\n",
    "\n",
    "LOSS_WEIGHT = [1., 1., 1.] # \n",
    "\n",
    "LEARNING_RATE = 1e-4  # best so far, dont change\n",
    "WEIGHT_DECAY = 3e-3  # best so far, dont change\n",
    "BETAS = (0.95, 0.98)  # this is the best for stable training, default is (0.99, 0.999), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e9ff3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run before trainning new model from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c7df5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from SelfAttention import SentimentSelfAttention\n",
    "\n",
    "dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, device=device)\n",
    "review_parser.build_vocab(ds_train)\n",
    "label_parser.build_vocab(ds_train)\n",
    "\n",
    "model_attention = SentimentSelfAttention(embeddings,\n",
    "                                         d_model=D_MODEL,\n",
    "                                         num_heads=NUM_HEADS,\n",
    "                                         dropout=DROPOUT,\n",
    "                                         kqv_dropout=KQV_DROPOUT,\n",
    "                                         two_layers=TWO_LAYERS,\n",
    "                                         freeze_embedding=FREEZE_EMB,\n",
    "                                         dense_dropout=DENSE_DROPOUT).to(device)\n",
    "\n",
    "att_optimizer = torch.optim.Adam(model_attention.parameters(), lr=LEARNING_RATE,\n",
    "                                 weight_decay=WEIGHT_DECAY, betas=BETAS)\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss(weight=torch.tensor(LOSS_WEIGHT).to(device))\n",
    "\n",
    "print(\"trainable params:\", \n",
    "      sum(p.numel() for p in model_attention.parameters() if p.requires_grad)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce423286",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run only when trainning model that has been saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee975d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Attention_res = train(model_attention, att_optimizer, loss_fn, dl_train, dl_test, max_epochs=200,\n",
    "          num_batches=500, save_path=\"./models/selfAttentionGlove200.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b128f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_attention = torch.load(\"./models/selfAttentionGlove200.pt\")\n",
    "att_optimizer = torch.optim.Adam(model_attention.parameters(), lr=LEARNING_RATE, \n",
    "                                 weight_decay=WEIGHT_DECAY, betas=BETAS)\n",
    "# loss_fn = torch.nn.NLLLoss(weight=torch.tensor([1., 1., 2.]).to(device))\n",
    "# model_attention.embedding_layer.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_fit(Attention_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d2d8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# get the best predictions from the model\n",
    "def get_preds_and_labels(model, dl, device):\n",
    "    model.eval()\n",
    "    all_preds = torch.tensor([]).to(device)\n",
    "    GT_labels = torch.tensor([]).to(device)\n",
    "\n",
    "    for i,batch in enumerate(dl):\n",
    "        X, y = batch\n",
    "        \n",
    "        _, preds = model(X)\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        \n",
    "        \n",
    "        all_preds = torch.cat((all_preds, preds), dim=0)\n",
    "        GT_labels = torch.cat((GT_labels, y), dim=0)\n",
    "    model.train()\n",
    "    print(all_preds)\n",
    "    return all_preds , GT_labels\n",
    "\n",
    "with torch.no_grad():\n",
    "    # baseline VS SelfAttention Confusion matrices\n",
    "    bl_best_preds, bl_GT_preds= get_preds_and_labels(model, dl_test, device)\n",
    "    se_best_preds, se_GT_preds = get_preds_and_labels(model_attention, dl_test, device)\n",
    "    \n",
    "    # Now lets demonstrate the confusion matrix of the test set.\n",
    "    IC = type('IdentityClassifier', (), {\"predict\": lambda i : i, \"_estimator_type\": \"classifier\"})\n",
    "    class_names = ['Positive','Negative','Neutral']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1 , figsize=(25, 10))\n",
    "    disp = plot_confusion_matrix(IC, bl_GT_preds.cpu(), bl_best_preds.cpu() ,display_labels=class_names, cmap=plt.cm.Blues,  ax=ax1);\n",
    "    disp.ax_.set_title('Baseline Confusion Matrix\\n')\n",
    "\n",
    "    disp = plot_confusion_matrix(IC, se_GT_preds.cpu(), se_best_preds.cpu() ,display_labels=class_names, cmap=plt.cm.Blues,  ax=ax2);\n",
    "    disp.ax_.set_title('Self-Attention Confusion Matrix\\n')\n",
    "\n",
    "# normalize='true',    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b87eb0",
   "metadata": {},
   "source": [
    "## Experiments:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2284525",
   "metadata": {},
   "source": [
    "test the affect of model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89f1ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for d_model in [50, 70, 100, 150, 200]:\n",
    "    dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "        (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, device=device)\n",
    "    review_parser.build_vocab(ds_train)\n",
    "    label_parser.build_vocab(ds_train)\n",
    "\n",
    "    model_attention = SentimentSelfAttention(embeddings,\n",
    "                                             d_model=D_MODEL,\n",
    "                                             num_heads=NUM_HEADS,\n",
    "                                             dropout=DROPOUT,\n",
    "                                             kqv_dropout=KQV_DROPOUT,\n",
    "                                             two_layers=TWO_LAYERS,\n",
    "                                             freeze_embedding=FREEZE_EMB).to(device)\n",
    "\n",
    "    att_optimizer = torch.optim.Adam(model_attention.parameters(), lr=LEARNING_RATE,\n",
    "                                     weight_decay=WEIGHT_DECAY, betas=BETAS)\n",
    "\n",
    "    loss_fn = torch.nn.NLLLoss(weight=torch.tensor([1., 1., 1.5]).to(device))\n",
    "    \n",
    "    plot.plot_fit(train(model_attention, att_optimizer, loss_fn, dl_train, dl_test, max_epochs=81,\n",
    "          num_batches=500, print_every=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d838e1c",
   "metadata": {},
   "source": [
    "test different numbers of heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9da5df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for h in [1, 2, 4, 10, 20]:\n",
    "    dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "        (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, device=device)\n",
    "    review_parser.build_vocab(ds_train)\n",
    "    label_parser.build_vocab(ds_train)\n",
    "\n",
    "    model_attention = SentimentSelfAttention(embeddings,\n",
    "                                             d_model=D_MODEL,\n",
    "                                             num_heads=NUM_HEADS,\n",
    "                                             dropout=DROPOUT,\n",
    "                                             kqv_dropout=KQV_DROPOUT,\n",
    "                                             two_layers=TWO_LAYERS,\n",
    "                                             freeze_embedding=FREEZE_EMB).to(device)\n",
    "\n",
    "    att_optimizer = torch.optim.Adam(model_attention.parameters(), lr=LEARNING_RATE,\n",
    "                                     weight_decay=WEIGHT_DECAY, betas=BETAS)\n",
    "\n",
    "    loss_fn = torch.nn.NLLLoss(weight=torch.tensor([1., 1., 1.]).to(device))\n",
    "    \n",
    "    plot.plot_fit(train(model_attention, att_optimizer, loss_fn, dl_train, dl_test, max_epochs=81,\n",
    "          num_batches=500, print_every=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a4934",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f517d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.name, p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec4057",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model_attention)\n",
    "print(loss_fn.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b80723",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del model_attention\n",
    "#del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe013786",
   "metadata": {},
   "source": [
    "test the affect of model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f966b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d_model in [50, 70, 100, 150, 200]:\n",
    "    dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "        (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, device=device)\n",
    "    review_parser.build_vocab(ds_train)\n",
    "    label_parser.build_vocab(ds_train)\n",
    "\n",
    "    model_attention = SentimentSelfAttention(embeddings,\n",
    "                                             d_model=D_MODEL,\n",
    "                                             num_heads=NUM_HEADS,\n",
    "                                             dropout=DROPOUT,\n",
    "                                             kqv_dropout=KQV_DROPOUT,\n",
    "                                             two_layers=TWO_LAYERS,\n",
    "                                             freeze_embedding=FREEZE_EMB).to(device)\n",
    "\n",
    "    att_optimizer = torch.optim.Adam(model_attention.parameters(), lr=LEARNING_RATE,\n",
    "                                     weight_decay=WEIGHT_DECAY, betas=BETAS)\n",
    "\n",
    "    loss_fn = torch.nn.NLLLoss(weight=torch.tensor([1., 1., 1.5]).to(device))\n",
    "    \n",
    "    plot.plot_fit(train(model_attention, att_optimizer, loss_fn, dl_train, dl_test, max_epochs=81,\n",
    "          num_batches=500, print_every=10), legend=\"d-model = \"+str(d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57fab6",
   "metadata": {},
   "source": [
    "test different numbers of heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in [1, 2, 4, 10, 20]:\n",
    "    dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "        (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, device=device)\n",
    "    review_parser.build_vocab(ds_train)\n",
    "    label_parser.build_vocab(ds_train)\n",
    "\n",
    "    model_attention = SentimentSelfAttention(embeddings,\n",
    "                                             d_model=D_MODEL,\n",
    "                                             num_heads=NUM_HEADS,\n",
    "                                             dropout=DROPOUT,\n",
    "                                             kqv_dropout=KQV_DROPOUT,\n",
    "                                             two_layers=TWO_LAYERS,\n",
    "                                             freeze_embedding=FREEZE_EMB).to(device)\n",
    "\n",
    "    att_optimizer = torch.optim.Adam(model_attention.parameters(), lr=LEARNING_RATE,\n",
    "                                     weight_decay=WEIGHT_DECAY, betas=BETAS)\n",
    "\n",
    "    loss_fn = torch.nn.NLLLoss(weight=torch.tensor([1., 1., 1.]).to(device))\n",
    "    \n",
    "    plot.plot_fit(train(model_attention, att_optimizer, loss_fn, dl_train, dl_test, max_epochs=81,\n",
    "          num_batches=500, print_every=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5327e",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038852ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.name, p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b965e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model_attention)\n",
    "print(loss_fn.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb94c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del model_attention\n",
    "#del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e4961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}