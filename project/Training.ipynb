{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8da009e-17a0-457e-ba53-9ecc22f1689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8322104-e3a5-40f9-a5fe-d172427bfe51",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loading the dataset & Pre trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d35c3c05-abc3-498b-b1e7-d1c601ea5124",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchtext.data\n",
    "import torchtext.datasets\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "from RNN import SentimentGRU\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device =\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9febd-20cf-47a4-90db-58540a80e467",
   "metadata": {},
   "source": [
    "load the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33a40907-4307-4e66-b790-0321d5bece64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training   samples: 8544\n",
      "Number of validation samples: 1101\n",
      "Number of test       samples: 2210\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "\n",
    "# torchtext Field objects parse text (e.g. a review) and create a tensor representation\n",
    "\n",
    "# This Field object will be used for tokenizing the movie reviews text\n",
    "review_parser = torchtext.data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    init_token='<sos>', eos_token='<eos>', dtype=torch.long,\n",
    "    tokenize='spacy', tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "# This Field object converts the text labels into numeric values (0,1,2)\n",
    "label_parser = torchtext.data.Field(\n",
    "    is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    ")\n",
    "\n",
    "# Load SST, tokenize the samples and labels\n",
    "# ds_X are Dataset objects which will use the parsers to return tensors\n",
    "ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "    review_parser, label_parser, root=data_dir\n",
    ")\n",
    "\n",
    "n_train = len(ds_train)\n",
    "print(f'Number of training   samples: {n_train}')\n",
    "print(f'Number of validation samples: {len(ds_valid)}')\n",
    "print(f'Number of test       samples: {len(ds_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1712b7-cdb4-4a03-a0a8-630151356969",
   "metadata": {},
   "source": [
    "As required, we'll use the pre-trained word embeddings of glove 6B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "726ceb40-f61c-4b16-9508-7ccc7e46086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary size is 40k, Embedding chosen size in 50\n",
    "vocab, embeddings = [],[]\n",
    "with open('./GloVe/glove.6B.50d.txt','rt',encoding='utf8') as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a8701-de8d-4d86-ae18-d58a8a9f1add",
   "metadata": {},
   "source": [
    "add padding and unknown tokens to the embeddings array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c89a6c24-7465-47de-a62d-d0fa9bf1e522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<unk>' 'the' ',' '.' 'of' 'to' 'and' 'in' 'a']\n"
     ]
    }
   ],
   "source": [
    "# Add the padding and the unknown tokens to the vocab and embeddings arrays\n",
    "\n",
    "vocab = np.array(vocab) \n",
    "embeddings = np.array(embeddings)\n",
    "vocab = np.insert(vocab, 0, '<pad>')\n",
    "vocab = np.insert(vocab, 1, '<unk>')\n",
    "\n",
    "unk_emb = np.mean(embeddings, axis=0, keepdims=True)\n",
    "pad_emb = np.zeros_like(embeddings[0]).reshape(1,-1)\n",
    "\n",
    "\n",
    "embeddings = np.vstack((pad_emb, unk_emb, embeddings))\n",
    "\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02664b4-3c2c-4a17-a804-1af0a1efaddc",
   "metadata": {},
   "source": [
    "## Baseline Model - Sentiment Analysis using RNN - LSTM\n",
    "\n",
    "As for the first part in our experiment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa7101a6-a28f-4e63-81eb-be20b528e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader, max_epochs=100,\n",
    "          num_batches=400, save_path=None):\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    \n",
    "    for epoch_idx in range(max_epochs):\n",
    "        total_loss, num_correct = 0, 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            X, y = batch.text, batch.label\n",
    "\n",
    "            # Forward pass\n",
    "            _, y_pred_log_proba = model(X)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred_log_proba, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "\n",
    "            if batch_idx == num_batches-1:\n",
    "                break\n",
    "        \n",
    "        curr_train_loss = total_loss /(num_batches)\n",
    "        curr_train_acc = num_correct /(num_batches*BATCH_SIZE)\n",
    "        train_losses.append(curr_train_loss)\n",
    "        train_acc.append(curr_train_acc)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch #{epoch_idx}, loss={curr_train_loss:.3f}, accuracy={curr_train_acc:.3f}, elapsed={time.time()-start_time:.1f} sec\")\n",
    "        \n",
    "        if save_path:\n",
    "            torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b61448d6-c2f0-4306-bc5e-07d260a4c154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60fe3ed6-7cb3-49d9-9f48-62da72a266ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, device=device)\n",
    "review_parser.build_vocab(ds_train)\n",
    "label_parser.build_vocab(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43176f64-511e-439b-acd9-c67bbe659388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentimentGRU(embeddings, hidden_size=HIDDEN_SIZE,\n",
    "                     num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9f2839b-5b4b-431a-9c27-bca7e09b795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentGRU(\n",
      "  (embedding_layer): Embedding(400002, 50)\n",
      "  (gru): GRU(50, 128)\n",
      "  (dense_linear): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"./models/sentimentGRU.pt\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfd65689-9837-4a9a-bc2d-1efae7e5b576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=0.561, accuracy=0.221, elapsed=1.5 sec\n",
      "Epoch #1, loss=0.560, accuracy=0.223, elapsed=1.3 sec\n",
      "Epoch #2, loss=0.560, accuracy=0.224, elapsed=1.3 sec\n",
      "Epoch #3, loss=0.560, accuracy=0.225, elapsed=1.3 sec\n",
      "Epoch #4, loss=0.560, accuracy=0.225, elapsed=1.3 sec\n",
      "Epoch #5, loss=0.560, accuracy=0.226, elapsed=1.4 sec\n",
      "Epoch #6, loss=0.560, accuracy=0.223, elapsed=1.4 sec\n",
      "Epoch #7, loss=0.560, accuracy=0.222, elapsed=1.3 sec\n",
      "Epoch #8, loss=0.557, accuracy=0.229, elapsed=1.3 sec\n",
      "Epoch #9, loss=0.554, accuracy=0.242, elapsed=1.3 sec\n",
      "Epoch #10, loss=0.548, accuracy=0.252, elapsed=1.3 sec\n",
      "Epoch #11, loss=0.536, accuracy=0.270, elapsed=1.3 sec\n",
      "Epoch #12, loss=0.513, accuracy=0.295, elapsed=1.3 sec\n",
      "Epoch #13, loss=0.475, accuracy=0.322, elapsed=1.3 sec\n",
      "Epoch #14, loss=0.413, accuracy=0.355, elapsed=1.4 sec\n",
      "Epoch #15, loss=0.336, accuracy=0.391, elapsed=1.4 sec\n",
      "Epoch #16, loss=0.269, accuracy=0.420, elapsed=1.4 sec\n",
      "Epoch #17, loss=0.208, accuracy=0.449, elapsed=1.3 sec\n",
      "Epoch #18, loss=0.139, accuracy=0.480, elapsed=1.4 sec\n",
      "Epoch #19, loss=0.102, accuracy=0.496, elapsed=1.3 sec\n",
      "Epoch #20, loss=0.073, accuracy=0.510, elapsed=1.4 sec\n",
      "Epoch #21, loss=0.062, accuracy=0.512, elapsed=1.3 sec\n",
      "Epoch #22, loss=0.041, accuracy=0.521, elapsed=1.4 sec\n",
      "Epoch #23, loss=0.031, accuracy=0.523, elapsed=1.4 sec\n",
      "Epoch #24, loss=0.036, accuracy=0.522, elapsed=1.3 sec\n",
      "Epoch #25, loss=0.025, accuracy=0.526, elapsed=1.3 sec\n",
      "Epoch #26, loss=0.020, accuracy=0.527, elapsed=1.3 sec\n",
      "Epoch #27, loss=0.029, accuracy=0.524, elapsed=1.3 sec\n",
      "Epoch #28, loss=0.023, accuracy=0.527, elapsed=1.3 sec\n",
      "Epoch #29, loss=0.021, accuracy=0.526, elapsed=1.4 sec\n",
      "Epoch #30, loss=0.016, accuracy=0.529, elapsed=1.4 sec\n",
      "Epoch #31, loss=0.023, accuracy=0.526, elapsed=1.4 sec\n",
      "Epoch #32, loss=0.019, accuracy=0.528, elapsed=1.3 sec\n",
      "Epoch #33, loss=0.021, accuracy=0.527, elapsed=1.4 sec\n",
      "Epoch #34, loss=0.015, accuracy=0.529, elapsed=1.3 sec\n",
      "Epoch #35, loss=0.024, accuracy=0.527, elapsed=1.3 sec\n",
      "Epoch #36, loss=0.019, accuracy=0.528, elapsed=1.3 sec\n",
      "Epoch #37, loss=0.014, accuracy=0.530, elapsed=1.3 sec\n",
      "Epoch #38, loss=0.017, accuracy=0.528, elapsed=1.3 sec\n",
      "Epoch #39, loss=0.015, accuracy=0.529, elapsed=1.3 sec\n",
      "Epoch #40, loss=0.019, accuracy=0.528, elapsed=1.4 sec\n",
      "Epoch #41, loss=0.023, accuracy=0.526, elapsed=1.3 sec\n",
      "Epoch #42, loss=0.013, accuracy=0.529, elapsed=1.4 sec\n",
      "Epoch #43, loss=0.008, accuracy=0.531, elapsed=1.3 sec\n",
      "Epoch #44, loss=0.020, accuracy=0.527, elapsed=1.4 sec\n",
      "Epoch #45, loss=0.021, accuracy=0.527, elapsed=1.4 sec\n",
      "Epoch #46, loss=0.017, accuracy=0.528, elapsed=1.3 sec\n",
      "Epoch #47, loss=0.012, accuracy=0.530, elapsed=1.3 sec\n",
      "Epoch #48, loss=0.011, accuracy=0.531, elapsed=1.3 sec\n",
      "Epoch #49, loss=0.009, accuracy=0.531, elapsed=1.3 sec\n",
      "Epoch #50, loss=0.022, accuracy=0.527, elapsed=1.3 sec\n",
      "Epoch #51, loss=0.013, accuracy=0.530, elapsed=1.4 sec\n",
      "Epoch #52, loss=0.008, accuracy=0.531, elapsed=1.3 sec\n",
      "Epoch #53, loss=0.012, accuracy=0.529, elapsed=1.4 sec\n",
      "Epoch #54, loss=0.016, accuracy=0.528, elapsed=1.3 sec\n",
      "Epoch #55, loss=0.018, accuracy=0.528, elapsed=1.3 sec\n",
      "Epoch #56, loss=0.010, accuracy=0.531, elapsed=1.3 sec\n",
      "Epoch #57, loss=0.009, accuracy=0.531, elapsed=1.3 sec\n",
      "Epoch #58, loss=0.015, accuracy=0.529, elapsed=1.3 sec\n",
      "Epoch #59, loss=0.023, accuracy=0.527, elapsed=1.4 sec\n",
      "Epoch #60, loss=0.016, accuracy=0.529, elapsed=1.3 sec\n",
      "Epoch #61, loss=0.007, accuracy=0.532, elapsed=1.3 sec\n",
      "Epoch #62, loss=0.005, accuracy=0.532, elapsed=1.3 sec\n",
      "Epoch #63, loss=0.013, accuracy=0.530, elapsed=1.3 sec\n",
      "Epoch #64, loss=0.013, accuracy=0.530, elapsed=1.3 sec\n",
      "Epoch #65, loss=0.024, accuracy=0.526, elapsed=1.3 sec\n",
      "Epoch #66, loss=0.012, accuracy=0.530, elapsed=1.3 sec\n",
      "Epoch #67, loss=0.008, accuracy=0.531, elapsed=1.4 sec\n",
      "Epoch #68, loss=0.011, accuracy=0.530, elapsed=1.3 sec\n",
      "Epoch #69, loss=0.016, accuracy=0.529, elapsed=1.4 sec\n",
      "Epoch #70, loss=0.006, accuracy=0.532, elapsed=1.3 sec\n",
      "Epoch #71, loss=0.005, accuracy=0.533, elapsed=1.3 sec\n",
      "Epoch #72, loss=0.019, accuracy=0.528, elapsed=1.3 sec\n",
      "Epoch #73, loss=0.025, accuracy=0.526, elapsed=1.4 sec\n",
      "Epoch #74, loss=0.013, accuracy=0.530, elapsed=1.3 sec\n",
      "Epoch #75, loss=0.005, accuracy=0.532, elapsed=1.3 sec\n",
      "Epoch #76, loss=0.002, accuracy=0.533, elapsed=1.3 sec\n",
      "Epoch #77, loss=0.001, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #78, loss=0.000, accuracy=0.534, elapsed=1.4 sec\n",
      "Epoch #79, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #80, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #81, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #82, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #83, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #84, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #85, loss=0.000, accuracy=0.534, elapsed=1.4 sec\n",
      "Epoch #86, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #87, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #88, loss=0.000, accuracy=0.534, elapsed=1.4 sec\n",
      "Epoch #89, loss=0.000, accuracy=0.534, elapsed=1.4 sec\n",
      "Epoch #90, loss=0.000, accuracy=0.534, elapsed=1.4 sec\n",
      "Epoch #91, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #92, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #93, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #94, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #95, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #96, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #97, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #98, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n",
      "Epoch #99, loss=0.000, accuracy=0.534, elapsed=1.3 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, loss_fn, dl_train, max_epochs=100,\n",
    "      num_batches=500, save_path = \"./models/sentimentGRU.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012991f-3054-4088-aa45-20b4cd837db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
