{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "$$\n",
    "\n",
    "# Part 3: Mini-Project\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you'll implement a small comparative-analysis project, heavily based on the materials from the tutorials and homework.\n",
    "\n",
    "You must **choose one** of the project options specified below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You should implement the code which displays your results in this notebook, and add any additional code files for your implementation in the `project/` directory. You can import these files here, as we do for the homeworks.\n",
    "- Running this notebook should not perform any training - load your results from some output files and display them here. The notebook must be runnable from start to end without errors.\n",
    "- You must include a detailed write-up (in the notebook) of what you implemented and how. \n",
    "- Explain the structure of your code and how to run it to reproduce your results.\n",
    "- Explicitly state any external code you used, including built-in pytorch models and code from the course tutorials/homework.\n",
    "- Analyze your numerical results, explaining **why** you got these results (not just specifying the results).\n",
    "- Where relevant, place all results in a table or display them using a graph.\n",
    "- Before submitting, make sure all files which are required to run this notebook are included in the generated submission zip.\n",
    "- Try to keep the submission file size under 10MB. Do not include model checkpoint files, dataset files, or any other non-essentials files. Instead include your results as images/text files/pickles/etc, and load them for display in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Self-Attention and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Tutorials 6 and 7, we'll implement and train an improved sentiment analysis model.\n",
    "We'll use self-attention instead of RNNs and incorporate pre-trained word embeddings.\n",
    "\n",
    "In tutorial 6 we saw that we can train word embeddings together with the model.\n",
    "Although this produces embeddings which are customized to the specific task at hand,\n",
    "it also greatly increases training time.\n",
    "A common technique is to use pre-trained word embeddings.\n",
    "This is essentially a large mapping from words (e.g. in english) to some\n",
    "high-dimensional vector, such that semantically similar words have an embedding that is\n",
    "\"close\" by some metric (e.g. cosine distance).\n",
    "Use the [GloVe](https://nlp.stanford.edu/projects/glove/) 6B embeddings for this purpose.\n",
    "You can load these vectors into the weights of an `nn.Embedding` layer.\n",
    "\n",
    "In tutorial 7 we learned how attention can be used to learn to predict a relative importance\n",
    "for each element in a sequence, compared to the other elements.\n",
    "Here, we'll replace the RNN with self-attention only approach similar to Transformer models, roughly based on [this paper](https://www.aclweb.org/anthology/W18-6219.pdf).\n",
    "After embedding each word in the sentence using the pre-trained word-embedding a positional-encoding vector is added to provide each word in the sentence a unique value based on it's location.\n",
    "One or more self-attention layers are then applied to the results, to obtain an importance weighting for each word.\n",
    "Then we classify the sentence based on the average these weighted encodings.\n",
    "\n",
    "\n",
    "Now, using these approaches, you need to:\n",
    "\n",
    "- Implement a **baseline** model: Use pre-trained embeddings with an RNN-based model.\n",
    "You can use LSTM/GRU or bi-directional versions of these, in a way very similar to what we implemented in the tutorial.\n",
    "-  Implement an **improved** model: Based on the self-attention approach, implement an attention-based sentiment analysis model that has 1-2 self-attention layers instead of an RNN. You should use the same pre-trained word embeddings for this model.\n",
    "- You can use pytorch's built-in RNNs, attention layers, etc.\n",
    "- For positional encoding you can use the sinosoidal approach described in the paper (first proposed [here](https://arxiv.org/pdf/1706.03762.pdf)). You can use existing online implementations (even though it's straightforward to implement). \n",
    "- You can use the SST database as shown in the tutorial.\n",
    "\n",
    "Your results should include:\n",
    "- Everything written in the **Guidelines** above.\n",
    "- A comparative analysis: compare the baseline to the improved model. Compare in terms of overall classification accuracy and show a multiclass confusion matrix.\n",
    "- Visualize of the attention maps for a few movie reviews from each class, and explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrally-Normalized Wasserstein GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW3 we implemented a simple GANs from scratch, using an approach very similar to the original GAN paper. However, the results left much to be desired and we discovered first-hand how hard it is to train GANs due to their inherent instability.\n",
    "\n",
    "One of the prevailing approaches for improving training stability for GANs is to use a technique called [Spectral Normalization](https://arxiv.org/pdf/1802.05957.pdf) to normalize the largest singular value of a weight matrix so that it equals 1.\n",
    "This approach is generally applied to the discriminator's weights in order to stabilize training. The resulting model is sometimes referred to as a SN-GAN.\n",
    "See Appendix A in the linked paper for the exact algorithm. You can also use pytorch's `spectral_norm`.\n",
    "\n",
    "Another very common improvement to the vanilla GAN is known a [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf) (WGAN). It uses a simple modification to the loss function, with strong theoretical justifications based on the Wasserstein (earth-mover's) distance.\n",
    "See also [here](https://developers.google.com/machine-learning/gan/loss) for a brief explanation of this loss function.\n",
    "\n",
    "One problem with generative models for images is that it's difficult to objectively assess the quality of the resulting images.\n",
    "To also obtain a quantitative score for the images generated by each model,\n",
    "we'll use the [Inception Score](https://arxiv.org/pdf/1606.03498.pdf).\n",
    "This uses a pre-trained Inception CNN model on the generated images and computes a score based on the predicted probability for each class.\n",
    "Although not a perfect proxy for subjective quality, it's commonly used a way to compare generative models.\n",
    "You can use an implementation of this score that you find online, e.g. [this one](https://github.com/sbarratt/inception-score-pytorch) or implement it yourself.\n",
    "\n",
    "Based on the linked papers, add Spectral Normalization and the Wassertein loss to your GAN from HW3.\n",
    "Compare between:\n",
    "- The baseline model (vanilla GAN)\n",
    "- SN-GAN (vanilla + Spectral Normalization)\n",
    "- WGAN (using Wasserstein Loss)\n",
    "- Optional: SN+WGAN, i.e. a combined model using both modifications.\n",
    "\n",
    "As a dataset, you can use [LFW](http://vis-www.cs.umass.edu/lfw/) as in HW3 or [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html), or even choose a custom dataset (note that there's a dataloder for CelebA in `torchvision`). \n",
    "\n",
    "Your results should include:\n",
    "- Everything written in the **Guidelines** above.\n",
    "- A comparative analysis between the baseline and the other models. Compare:\n",
    "  - Subjective quality (show multiple generated images from each model)\n",
    "  - Inception score (can use a subset of the data).\n",
    "- You should show substantially improved subjective visual results with these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: This is where you should write your explanations and implement the code to display the results.\n",
    "See guidelines about what to include in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Transformers became very frequent in the last fiew years in the field of Deep Learning and especially in NLP & Signal processing domains.\n",
    "One of the main mechanisms in the transformer's architecture as proposed by Vswani et al.,2017 is the self attention mechanism. That mechanism was proposed as modification to te RNN- based architectures due to its ability to improve modeling of long range dependencies.\n",
    "In this Mini-Project we will Implement the RNN based model as a baseline and compare it with the model as proposed in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "In the last few years , since the Transformers model was proposed in the paper \"Attention is all you need\" [Vaswani. et al., 2017], it started to pop up in many uses in Deep learning applications in the fields of NLP & signal processing analysis.\n",
    "The core of this model is the Self-Attention mechanism used to replace the RNN use and overcome its drawbacks.\n",
    "The self Attention mechanism allows us to parallelize much of the computations that are done in RNNs.\n",
    "\n",
    "|<img src=\"project/Images/RNNvsSelfAttention.png\" height=\"50\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "We implemented the GRU model in the file /project/RNN.py, and the self-attention model in the file /project/SelfAttention.py.\n",
    "\n",
    "The Self attention model can be either with one self attention layer or with two layers. This is configured by the *two_layers* boolean parameter passed to *\\_\\_init\\_\\_*.\n",
    "The model gets an input of tokenized sequence and applies:\n",
    "* a fine tuned embedding layer + Dropout\n",
    "* 3 linear layers + relu + batchnorm to create keys, values and queries of dimention d_model. \n",
    "* pytorch built in attention layer with dropout\n",
    "* linear + relu + batchnorm + dropout layers on the average of the attention output\n",
    "* in case of *two_layers=True* 3 linear layers with skip connection in order to create new keys, values and queries and then to another attention layer with feed forward after it. the whole second attention block is a residual block with a skip connection\n",
    "* At the output we have a dense linar layer with 3 outputs, one for each class score.\n",
    "\n",
    "Most of the above is according to the paper. We have decided to add skip connection and batchnormalization because we encounterd vanishing gradients during training. \n",
    "There are dropout layers after the key, values, queries and the final feedforward, but they are set to *p=0* and can be set with seperate dropout through *__init__*. in our trained model we didn't use them because it did not show any improvement and they are not used in the model that introduced in the paper as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentGRU(\n",
      "  (embedding_layer): Embedding(2, 2)\n",
      "  (gru): GRU(2, 128, num_layers=2)\n",
      "  (dense_linear): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from project.RNN import SentimentGRU\n",
    "\n",
    "# dummy embeddings just to print the model:\n",
    "embeddings = numpy.zeros([2,2])\n",
    "\n",
    "model_gru = SentimentGRU(embeddings)\n",
    "print(model_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentSelfAttention(\n",
      "  (embedding_layer): Embedding(2, 2)\n",
      "  (PositionalEncoding): PositionalEncoding()\n",
      "  (embeddings_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (q_feedforward): Sequential(\n",
      "    (0): LinearWithBN(\n",
      "      (lin): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (k_feedforward): Sequential(\n",
      "    (0): LinearWithBN(\n",
      "      (lin): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (v_feedforward): Sequential(\n",
      "    (0): LinearWithBN(\n",
      "      (lin): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (SelfAttention1): MultiheadAttention(\n",
      "    (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (attention_out_feedforward): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (dense_linear): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=3, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from project.SelfAttention import SentimentSelfAttention\n",
    "model_attention = SentimentSelfAttention(embeddings)\n",
    "print(model_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We have trained both models for sentiment analaysis on the SST data set with the 3 classes version. \n",
    "The accuracies on the test set are computed on the *AnalysisAndVisualization.ipynb* notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "The GRU model achived accuracy of 62%.\n",
    "We used two layers with 100 units each and a final linear layer to project to 3 classes. The model easily managed to overfit the train set, so we used dropout which significantly improved the results on the test set. \n",
    "We show the confusion matrix below with farther explaination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention\n",
    "\n",
    "We trained the self-attention model in serveral setups, performed different experiments and managed to get accuracy of 66%.\n",
    "The final model has 1 Attention layer with d_model of 100 (which means we project the embedding dimention of 200 to 100) and 1 head.\n",
    "The model shows a bit more superior results then a GRU, but yet not a very high accuracy. That is probably due to the self attention that did not work perfectly as we will see later. \n",
    "\n",
    "We have found out that:\n",
    "\n",
    "* For this task, a two layer attention model did not show any better results then 1 layer. Both easily managed to overfit the train set (100% accuracy) with a models and did not manage to pass 66% accuracy on the test set even with extreamly high dropout (>0.9) and L2 regularization. \n",
    "\n",
    "* A releativly small model can achive the best result, the model itself has less then 0.5 Milion parameters while the embedding layer has 80 Milion\n",
    "\n",
    "* By expirementing with different number of self attention heads, we didn't see any significant affect for this task, so we stayed with 1 head \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imbalance And Bias\n",
    "The amount of samples in both the train and the test set of neutral samples is lower then positive and negative. That made our models to be biased and predict almost all of the neutral samples as positive or negative. We have tried to fix that by giving higher weight to the neutral class in the loss function but it did'nt help a lot. We also tried to overweight the neutral class, giving it weight which is much higher then the inverse of its part in the dataset, it made the model give more neutral predictions, but it harmed the accuracy on positive and negative samples and the model ended up having lower accuracy. The best result was achived without changing the weight of the classes in the loss function. \n",
    "\n",
    "Other reason for this bias, may be the fact that the task is not a classic classification task. There are ordinal relations between classes: $positive > neutral > negative$. We think that since neutral the class is close to the two other classes, it got mixed with them and almost \"disappeared\". A solution for that may be using ordinal classification methods.\n",
    "\n",
    "These results are shown well in the confusion matrices of both models:\n",
    "\n",
    "<img src=\"project/Images/ConfusionMat.JPG\" style=\"width:1000px;\">\n",
    "\n",
    "**Both matrices are for the models with the best accuracy**, and it is shown that none of the neutral samples are classified correctly.\n",
    "Actually, most of the model error comes from neutral samples misclassification, and it would be the first target to improve our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization\n",
    "Here are some examples of the attention weights from our model. Some general conclutions about it are in the end.\n",
    "\n",
    "\n",
    "<img src=\"project/Images/AttentionVis1.JPG\" style=\"width:500px\">\n",
    "Here we can see that the weight of the corresponding word for wonderful in the output sequence is highly affected by character. That shows that the model managed to connect the \n",
    "adjective to the noun. We can also see the character is affected by the word 'a', which is actualy a mistake, since 'a' adresses the word comedy in this sentence. This is actualy a complex case where the model is probably wrong. \n",
    "\n",
    "In addition, the word 'a' should not affect the sentiment too match since it does not change the meaning of the sentence and we might actualy improve our results by cleaning such words from our data.\n",
    "\n",
    "___\n",
    "\n",
    "<img src=\"project/Images/AttentionVis2.JPG\" style=\"width:500px\">\n",
    "Here we can see a strong connection between both and awful, and it makes sense but there are missing connections we would expect like between awful and appealing. The writer used 2 adjectives, one positive and one negative, and if the model would have understood that 'both' adresses those two adjectives it might have understood that this review is neutral. \n",
    "\n",
    "___\n",
    "<img src=\"project/Images/AttentionVis3.JPG\" style=\"width:500px\">\n",
    "The strongest connection here is between film and rendered, which is a correct connection but it doesn't help alot for the sentiment analysis like the word beutiful.\n",
    "\n",
    "___\n",
    "After looking at all the above examples, we noticed that there are tokens which may confuse the model and add randomness such as '.' and mainly '\\<sos\\>' and '\\<eos\\>' which we added artifitialy. The '\\<sos\\>' and '\\<eos\\>' are not needed for this task but they have random weights in each review, we might have gotten better results without adding them to the vocabulary. It might have been also helpful to remove tokens such as '.' and 'a' from the reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
